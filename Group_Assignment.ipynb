{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7ece36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0  fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "0           0            5.4              0.29         0.38             1.2   \n",
      "1           1            6.7              0.24         0.29            14.9   \n",
      "2           2            6.8              0.33         0.31             7.4   \n",
      "3           3            6.4              0.27         0.19             2.0   \n",
      "4           4            6.1              0.30         0.30             2.1   \n",
      "\n",
      "   chlorides  free sulfur dioxide  total sulfur dioxide  density    pH  \\\n",
      "0      0.029                 31.0                 132.0  0.98895  3.28   \n",
      "1      0.053                 55.0                 136.0  0.99839  3.03   \n",
      "2      0.045                 34.0                 143.0  0.99226  3.06   \n",
      "3      0.084                 21.0                 191.0  0.99516  3.49   \n",
      "4      0.031                 50.0                 163.0  0.98950  3.39   \n",
      "\n",
      "   sulphates  alcohol  quality  \n",
      "0       0.36     12.4        6  \n",
      "1       0.52      9.0        5  \n",
      "2       0.55     12.2        6  \n",
      "3       0.63      9.6        4  \n",
      "4       0.43     12.7        7  \n"
     ]
    }
   ],
   "source": [
    "# Q1\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"sparklingwine.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81075f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>quality</th>\n",
       "      <th>good wine</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   quality  good wine\n",
       "0        6          1\n",
       "1        5          0\n",
       "2        6          1\n",
       "3        4          0\n",
       "4        7          1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Q2\n",
    "\n",
    "df[\"good wine\"] = (df[\"quality\"] >= 6).astype(int)\n",
    "\n",
    "df[[\"quality\", \"good wine\"]].head()\n",
    "\n",
    "# df = df.drop(columns=[\"Unnamed: 0\"]) # remove Unnamed: 0 column (not a feature)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4f9277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 300 400\n"
     ]
    }
   ],
   "source": [
    "#Q3\n",
    "\n",
    "train_df = df.iloc[:900]\n",
    "val_df = df.iloc[900:1200]\n",
    "test_df = df.iloc[1200:1600]\n",
    "\n",
    "print(len(train_df), len(val_df), len(test_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea9e9d1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0              0.0\n",
      "fixed acidity          -0.0\n",
      "volatile acidity        0.0\n",
      "citric acid             0.0\n",
      "residual sugar         -0.0\n",
      "chlorides              -0.0\n",
      "free sulfur dioxide    -0.0\n",
      "total sulfur dioxide    0.0\n",
      "density                -0.0\n",
      "pH                     -0.0\n",
      "sulphates              -0.0\n",
      "alcohol                -0.0\n",
      "quality                 0.0\n",
      "dtype: float64\n",
      "Unnamed: 0              1.0\n",
      "fixed acidity           1.0\n",
      "volatile acidity        1.0\n",
      "citric acid             1.0\n",
      "residual sugar          1.0\n",
      "chlorides               1.0\n",
      "free sulfur dioxide     1.0\n",
      "total sulfur dioxide    1.0\n",
      "density                 1.0\n",
      "pH                      1.0\n",
      "sulphates               1.0\n",
      "alcohol                 1.0\n",
      "quality                 1.0\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "#Q4\n",
    "X_train = train_df.drop(columns=[\"good wine\"])\n",
    "y_train = train_df[\"good wine\"]\n",
    "\n",
    "X_val = val_df.drop(columns=[\"good wine\"])\n",
    "y_val = val_df[\"good wine\"]\n",
    "\n",
    "X_test = test_df.drop(columns=[\"good wine\"])\n",
    "y_test = test_df[\"good wine\"]\n",
    "\n",
    "mu = X_train.mean()\n",
    "sigma = X_train.std()\n",
    "\n",
    "# normalisation\n",
    "X_train_z = (X_train - mu) / sigma\n",
    "X_val_z   = (X_val   - mu) / sigma\n",
    "X_test_z  = (X_test  - mu) / sigma\n",
    "\n",
    "#print(X_train_z.mean())\n",
    "#print(X_train_z.std())\n",
    "\n",
    "print(X_train_z.mean().round(2))\n",
    "print(X_train_z.std().round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c761fa3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q5\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# train kNN models for k=1 to 100\n",
    "knn_models = {} # sore in dictionary\n",
    "\n",
    "for k in range(1, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_z, y_train) # train classifier on the normalised training data\n",
    "    knn_models[k] = knn # store trained model in dictionary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "43538741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9, 0.9133333333333333)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q6\n",
    "# evaluate in validation data\n",
    "val_accuracy = {} # dictionary to store classification accuracy of validation set\n",
    "\n",
    "for k in range(1, 101):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_z, y_train) # train the model using the normalised training data\n",
    "    val_accuracy[k] = knn.score(X_val_z, y_val) # return mean classification accuracy w score()\n",
    "\n",
    "# best classifier\n",
    "best_k = max(val_accuracy, key=val_accuracy.get) # max validation accuracy\n",
    "best_k, val_accuracy[best_k] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "144153f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07499999999999996"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q7\n",
    "\n",
    "# train using training and validation data\n",
    "# Combine training and validation data\n",
    "X_trainval_z = pd.concat([X_train_z, X_val_z])\n",
    "y_trainval = pd.concat([y_train, y_val])\n",
    "\n",
    "# Train final kNN classifier with the chosen k\n",
    "final_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "final_knn.fit(X_trainval_z, y_trainval)\n",
    "\n",
    "\n",
    "# predict on test data\n",
    "y_test_pred = final_knn.predict(X_test_z)\n",
    "\n",
    "\n",
    "# test accuracy\n",
    "test_accuracy = final_knn.score(X_test_z, y_test)\n",
    "test_accuracy\n",
    "\n",
    "\n",
    "# convert accuracy to generalisation error\n",
    "generalisation_error = 1 - test_accuracy\n",
    "generalisation_error\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32fcb650",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0             -0.0\n",
      "fixed acidity          -0.0\n",
      "volatile acidity       -0.0\n",
      "citric acid            -0.0\n",
      "residual sugar         -0.0\n",
      "chlorides              -0.0\n",
      "free sulfur dioxide     0.0\n",
      "total sulfur dioxide    0.0\n",
      "density                -0.0\n",
      "pH                     -0.0\n",
      "sulphates               0.0\n",
      "alcohol                 0.0\n",
      "quality                -0.0\n",
      "dtype: float64\n",
      "Unnamed: 0              1.0\n",
      "fixed acidity           1.0\n",
      "volatile acidity        1.0\n",
      "citric acid             1.0\n",
      "residual sugar          1.0\n",
      "chlorides               1.0\n",
      "free sulfur dioxide     1.0\n",
      "total sulfur dioxide    1.0\n",
      "density                 1.0\n",
      "pH                      1.0\n",
      "sulphates               1.0\n",
      "alcohol                 1.0\n",
      "quality                 1.0\n",
      "dtype: float64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.14500000000000002"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q8\n",
    "train_df2 = df.iloc[:400]\n",
    "val_df2 = df.iloc[400:800]\n",
    "test_df2 = df.iloc[800:1600]\n",
    "\n",
    "X_train2 = train_df2.drop(columns=[\"good wine\"])\n",
    "y_train2 = train_df2[\"good wine\"]\n",
    "\n",
    "X_val2 = val_df2.drop(columns=[\"good wine\"])\n",
    "y_val2 = val_df2[\"good wine\"]\n",
    "\n",
    "X_test2 = test_df2.drop(columns=[\"good wine\"])\n",
    "y_test2 = test_df2[\"good wine\"]\n",
    "\n",
    "mu2 = X_train2.mean()\n",
    "sigma2 = X_train2.std()\n",
    "\n",
    "# normalisation\n",
    "X_train_z2 = (X_train2 - mu2) / sigma2\n",
    "X_val_z2 = (X_val2  - mu2) / sigma2\n",
    "X_test_z2  = (X_test2  - mu2) / sigma2\n",
    "\n",
    "print(X_train_z2.mean().round(2))\n",
    "print(X_train_z2.std().round(2))\n",
    "\n",
    "# train kNN models for k=1 to 100\n",
    "knn_models2 = {} # sore in dictionary\n",
    "\n",
    "for k in range(1, 101):\n",
    "    knn2 = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn2.fit(X_train_z2, y_train2) # train classifier on the normalised training data\n",
    "    knn_models2[k] = knn2 # store trained model in dictionary\n",
    "\n",
    "\n",
    "val_accuracy2 = {} # dictionary to store classification accuracy of validation set\n",
    "\n",
    "for k in range(1, 101):\n",
    "    knn2 = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn2.fit(X_train_z2, y_train2) # train the model using the normalised training data\n",
    "    val_accuracy2[k] = knn2.score(X_val_z2, y_val2) # return mean classification accuracy w score()\n",
    "\n",
    "# best classifier\n",
    "best_k2 = max(val_accuracy2, key=val_accuracy2.get) # max validation accuracy\n",
    "best_k2, val_accuracy2[best_k2] \n",
    "\n",
    "# train using training and validation data\n",
    "# Combine training and validation data\n",
    "X_trainval_z2 = pd.concat([X_train_z2, X_val_z2])\n",
    "y_trainval2 = pd.concat([y_train2, y_val2])\n",
    "\n",
    "# Train final kNN classifier with the chosen k\n",
    "final_knn2 = KNeighborsClassifier(n_neighbors=best_k2)\n",
    "final_knn2.fit(X_trainval_z2, y_trainval2)\n",
    "\n",
    "\n",
    "# predict on test data\n",
    "y_test_pred2 = final_knn2.predict(X_test_z2)\n",
    "\n",
    "\n",
    "# test accuracy\n",
    "test_accuracy2 = final_knn2.score(X_test_z2, y_test2)\n",
    "test_accuracy2\n",
    "\n",
    "\n",
    "# convert accuracy to generalisation error\n",
    "generalisation_error2 = 1 - test_accuracy2\n",
    "generalisation_error2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cf2f15",
   "metadata": {},
   "source": [
    "0.07499999999999996 vs 0.14500000000000002\n",
    "\n",
    "The generalisation error increases from approximately 0.075 to 0.145 when the training set size is reduced, indicating worse performance with less training data.\n",
    "\n",
    "This sensitivity reflects the data-driven nature of kNN, which relies heavily on having many nearby training points to make accurate predictions.\n",
    "\n",
    "With more training data (first split), the classifier generalises better and produces more reliable predictions on unseen data.\n",
    "\n",
    "With fewer training samples (second split), the classifier struggles to capture the underlying structure of the data, leading to higher test error.\n",
    "\n",
    "Overall, the classifier is reasonably well-suited to the dataset when sufficient training data are available, but its performance degrades noticeably when the training set is smaller.\n",
    "\n",
    "---\n",
    "\n",
    "Using the first data split (900 training, 300 validation, 400 test), the estimated generalisation error is approximately 0.075, whereas under the second split (400 training, 400 validation, 800 test) the generalisation error increases to approximately 0.145. This difference is substantial and suggests that the classifierâ€™s performance is sensitive to the amount of training data available. With fewer training samples, the kNN classifier is less able to accurately capture the underlying structure of the data, leading to poorer generalisation on unseen observations. This behaviour is expected for kNN, which is a non-parametric, data-driven method whose performance improves as the size of the training set increases.\n",
    "\n",
    "The classifier can be judged as reasonably well-suited to the data set in the first split, where the larger training set leads to a relatively low test error and stable performance. However, the marked increase in generalisation error under the second split suggests that the classifier relies heavily on having sufficient training data and may not generalise as reliably when data are scarce. Overall, this indicates that while kNN can perform well for this problem, its effectiveness depends strongly on the size of the training set, and care must be taken when deploying it in settings with limited labelled data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
